# Lesson 02 - CNN与ResNet

> 2026.2.1

---

## 卷积神经网络(CNN)

### 全连接网络处理图像的三大问题

1. **参数爆炸**：224×224×3图像 → 全连接层参数巨大（如150,528×1000 ≈ 1.5亿参数）
2. **忽略空间信息**：像素被独立处理，失去局部相关性
3. **平移不变性缺失**：同一物体在不同位置需要重新学习

### CNN的解决方案

| 问题 | CNN解决方案 | 说明 |
|------|------------|------|
| 参数爆炸 | **局部连接** | 每个神经元只连接局部区域（如3×3），而非所有像素 |
| 空间信息丢失 | **参数共享** | 同一个卷积核在整个图像上滑动，同一组权重检测所有位置的相同特征 |
| 平移不变性 | **卷积操作** | 相同特征在不同位置被相同卷积核检测 |

**参数对比示例**：
- 全连接参数：150,528 × 1000 ≈ **1.5亿**参数
- 卷积层(3×3卷积核, 64个)：3×3×3×64 = **1,728**参数

---

## 卷积层

### 卷积公式

$$
O_{y,x,c} = \sum_{i=0}^{K-1} \sum_{j=0}^{K-1} \sum_{d=0}^{C_{in}-1} W_{i,j,d,c} \cdot I_{y \cdot s+i, \, x \cdot s+j, \, d} + b_c
$$

**变量说明**：

| 符号 | 含义 | 说明 |
|------|------|------|
| $O_{y,x,c}$ | 输出特征图 | 位置 $(y,x)$、第 $c$ 个输出通道的值 |
| $I_{y,x,d}$ | 输入特征图 | 位置 $(y,x)$、第 $d$ 个输入通道的值 |
| $W_{i,j,d,c}$ | 卷积核权重 | 卷积核位置 $(i,j)$、输入通道 $d$、输出通道 $c$ |
| $b_c$ | 偏置项 | 第 $c$ 个输出通道的偏置 |
| $K$ | 卷积核大小 | 如 $K=3$ 表示 $3 \times 3$ 卷积核 |
| $s$ | 步长 (Stride) | 卷积核每次移动的像素数 |
| $C_{in}$ | 输入通道数 | 如RGB图像 $C_{in}=3$ |
| $C_{out}$ | 输出通道数 | 卷积核的个数 |

### 尺寸关系

**张量维度**：
| 张量 | 尺寸 | 说明 |
|------|------|------|
| 输入 $I$ | $H_{in} \times W_{in} \times C_{in}$ | 高度 × 宽度 × 通道数 |
| 卷积核 $W$ | $K \times K \times C_{in} \times C_{out}$ | 核大小 × 输入通道 × 输出通道 |
| 偏置 $b$ | $C_{out}$ | 每个输出通道一个偏置 |
| 输出 $O$ | $H_{out} \times W_{out} \times C_{out}$ | 高度 × 宽度 × 通道数 |

**输出尺寸计算公式**：

$$
H_{out} = \left\lfloor \frac{H_{in} + 2P - K}{S} \right\rfloor + 1
$$

$$
W_{out} = \left\lfloor \frac{W_{in} + 2P - K}{S} \right\rfloor + 1
$$

| 符号 | 含义 |
|------|------|
| $H_{in}, W_{in}$ | 输入的高度和宽度 |
| $H_{out}, W_{out}$ | 输出的高度和宽度 |
| $K$ | 卷积核大小 |
| $S$ | 步长 (Stride) |
| $P$ | 填充 (Padding) |
| $\lfloor \cdot \rfloor$ | 向下取整 |

### 关键超参数

| 超参数 | 说明 | 常用值 |
|--------|------|--------|
| **卷积核大小(K)** | 感受野大小 | 3×3（VGG）、1×1、7×7（早期） |
| **步长(Stride)** | 卷积核移动步长 | 1或2 |
| **填充(Padding)** | 边缘补零 | Valid/Same |

### 填充类型

- **Valid**：无填充（输出尺寸缩小）
- **Same**：填充使输出尺寸与输入相同

---

## 池化层
### 池化的作用

| 功能 | 比喻 | 效果 |
|------|------|------|
| **降维/下采样** | 看画时离远一点 | 减少计算量，加速训练 |
| **平移不变性** | 无论猫在画面左还是右，都知道是猫 | 增强模型鲁棒性 |
| **特征压缩** | 用"有很多船"代替数清每艘船 | 提取主要特征，忽略细节 |
| **扩大感受野** | 看整体布局而不是局部细节 | 让后续层看到更大范围 |
 

### 池化公式

**最大池化 (Max Pooling)**：

$$
O_{y,x,c} = \max_{0 \le i,j < K} I_{y \cdot s + i, \, x \cdot s + j, \, c}
$$

**平均池化 (Average Pooling)**：

$$
O_{y,x,c} = \frac{1}{K^2} \sum_{i=0}^{K-1} \sum_{j=0}^{K-1} I_{y \cdot s + i, \, x \cdot s + j, \, c}
$$

**变量说明**：

| 符号 | 含义 |
|------|------|
| $O_{y,x,c}$ | 输出特征图在位置 $(y,x)$、通道 $c$ 的值 |
| $I_{y,x,c}$ | 输入特征图在位置 $(y,x)$、通道 $c$ 的值 |
| $K$ | 池化窗口大小（如 $K=2$ 表示 $2 \times 2$） |
| $s$ | 步长，通常 $s = K$ |

**示例**（$2 \times 2$ 最大池化，步长为2）：

```
输入:                    输出:
[1, 4, 2, 8]
[3, 9, 5, 7]    →    [9, 8]
[2, 6, 4, 1]         [6, 7]
[5, 3, 7, 2]
```

### 池化类型

| 类型 | 说明 | 特点 |
|------|------|------|
| **最大池化(Max Pooling)** | 取窗口内最大值 | 最常用，保留最显著特征 |
| **平均池化(Average Pooling)** | 取窗口内平均值 | 保留平均特征强度 |
| **全局池化(Global Pooling)** | 整个特征图压缩成一个值 | 常用于CNN最后一层，代替全连接层，减少参数 |




## ResNet (残差网络)

### 深度网络的问题：梯度消失/爆炸

**链式法则求梯度**：

$$
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial h_n} \cdot \frac{\partial h_n}{\partial h_{n-1}} \cdots \frac{\partial h_2}{\partial h_1} \cdot \frac{\partial h_1}{\partial W_1}
$$

**变量说明**：

| 符号 | 含义 |
|------|------|
| $L$ | 损失函数 (Loss) |
| $W_1$ | 第1层的权重参数 |
| $y$ | 网络最终输出 |
| $h_i$ | 第 $i$ 层的隐藏层输出 |
| $\frac{\partial L}{\partial W_1}$ | 损失对第1层权重的梯度 |

**问题**：当网络很深时，很多 $\frac{\partial h_{i+1}}{\partial h_i} < 1$ 连乘 → 梯度趋近于0（梯度消失）

**实验现象**：
- 56层网络比20层网络训练误差和测试误差都更大
- 不是过拟合（训练误差也更高）
- 问题：网络变深后，优化变得困难

### 残差学习

**传统网络 vs 残差网络**：

| 网络类型 | 学习目标 | 公式 |
|----------|----------|------|
| 传统网络 | 直接学习映射 $H(x)$ | $y = H(x)$ |
| 残差网络 | 学习残差 $F(x)$ | $y = F(x) + x$ |

$$
\text{残差} = F(x) = H(x) - x
$$

**变量说明**：

| 符号 | 含义 |
|------|------|
| $x$ | 残差块的输入 |
| $y$ | 残差块的输出 |
| $H(x)$ | 期望学习的理想映射 |
| $F(x)$ | 残差函数，即 $H(x) - x$ |

> **核心思想**：网络只需学习输入与输出的"差异"（残差），而不是完整的映射

### 残差连接如何解决梯度消失

**传统网络的梯度**：

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial H(x)}{\partial x}
$$

**残差网络的梯度**（由于 $y = F(x) + x$）：

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial (F(x) + x)}{\partial x} = \frac{\partial L}{\partial y} \cdot \left( \frac{\partial F(x)}{\partial x} + 1 \right)
$$

| 符号 | 含义 |
|------|------|
| $\frac{\partial L}{\partial x}$ | 损失对输入的梯度 |
| $\frac{\partial F(x)}{\partial x}$ | 残差函数对输入的梯度 |
| $+1$ | 恒等映射的梯度（梯度高速公路） |

> **关键**：即使 $\frac{\partial F(x)}{\partial x} \approx 0$，梯度仍可通过 $+1$ 这条路径传播，避免梯度消失

### ResNet变体

| 模型 | 层数 |
|------|------|
| ResNet-34 | 34层 |
| ResNet-50 | 50层 |
| ResNet-101 | 101层 |
| ResNet-152 | 152层 |

---

## Homework

### 作业一：卷积层相关问题
1. 步长过大过小会导致什么？
2. 填充作用是干什么的，不加填充可以吗？
3. VGG怎么做到的3×3是最优的选择？

### 作业二：池化实现
- 手搓最大池化、平均池化

### 作业三：ResNet实现
- 手撕ResNet

### 大作业：CIFAR-10图像分类实战
- 在CIFAR-10上训练，达到90%+准确率
- 提交到GitHub，必须有训练结果截图
- 最低要求：80%以上准确率，训练10轮，提供训练截图
