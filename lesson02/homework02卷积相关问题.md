# 作业二：卷积相关问题回答



## 问题一：步长（stride）过大或过小会导致什么？

- **步长过大**：输出特征图尺寸迅速缩小，导致空间信息丢失、细节特征被跳过，模型定位能力下降；计算量和参数可能减少，但性能可能受损。
- **步长过小**（如恒为1）：输出尺寸保持较大，保留更多细节，但计算量和内存开销显著增加，训练速度变慢；感受野增长缓慢，可能需要更多层来覆盖同样的上下文。
- **折中建议**：常用步长为 $1$ 或 $2$。分类/检测网络中，靠池化或步长为2的卷积逐步下采样以平衡精度与计算量。

## 问题二：填充（padding）作用是什么？不加填充可以吗？

- **作用**：填充用于控制输出尺寸并保留边界信息，避免每次卷积都使特征图变小；常见类型有 `Valid`（无填充）和 `Same`（填充使输出与输入尺寸相同）。
- **为什么常用**：通过在边界补零，卷积核能覆盖输入的边缘像素，使边缘信息被同等处理；同时可保持空间分辨率、方便堆叠更多层而不至于过快缩小。
- **不加填充的后果**：不加填充（Valid）会导致输出逐层缩小、边缘信息丢失，在需要保持尺寸或精细定位时通常不可取；某些场景（如极小网络或特定结构）可接受但需注意感受野与输出尺寸变化。

## 问题三：VGG 为什么选择 $3\times3$ 卷积核被认为是“最优”的选择？

1. **等效感受野与参数效率**：多个 $3\times3$ 卷积堆叠可以构成更大的感受野且参数更少并引入更多非线性。例如，两个连续的 $3\times3$ 等效于一个 $5\times5$ 的感受野，但参数更少：

2. 单层 $5\times5$ 的参数量为 $25C^2$（假设通道数为 $C$）；两层 $3\times3$ 的参数量为 $2\times9C^2=18C^2$，参数更少且有更多的 ReLU 非线性变换，表达能力更强。
3. **更深的非线性表示**：堆叠两个或三个 $3\times3$ 在层间加了更多激活函数（如 ReLU），提升网络的表达能力和拟合复杂函数的能力。
4. **实现与计算友好**：$3\times3$ 卷积在现代深度学习框架和硬件（如 GPU）上高度优化，计算效率好且易于组合。



